{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685d109",
   "metadata": {},
   "source": [
    "# Data Processing script v2 for the SWEML v2.0\n",
    "This .ipynb script uses python module for processing predownloaded NASA ASO observations by Water Year, locating nearest SNOTEL sites, connecting SNOTEL obs with ASO obs, and add geospatial features to the ML training/testing/hindcast dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1ee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "HOME = os.getcwd()\n",
    "\n",
    "#If you get a proj.db error below, run the following and put the following into the terminal\n",
    "import pyproj\n",
    "# Get the PROJ data directory\n",
    "proj_data_dir = pyproj.datadir.get_data_dir()\n",
    "proj_db_path = proj_data_dir + \"/proj.db\"\n",
    "os.environ['PROJ_LIB'] =pyproj.datadir.get_data_dir()\n",
    "os.environ['PROJ_LIB']\n",
    "#set multiprocessing limits\n",
    "CPUS = len(os.sched_getaffinity(0))\n",
    "CPUS = int((CPUS/2)-2)\n",
    "\n",
    "#set home to the head of the SWEMLv2.0 directory\n",
    "HOME = os.chdir('..')\n",
    "HOME = os.getcwd()\n",
    "\n",
    "#Add your module here\n",
    "from utils.ASOget import ASODataProcessing_v2\n",
    "import utils.get_InSitu_obs as get_InSitu_obs\n",
    "import utils.GeoDF as GeoDF \n",
    "import utils.Obs_to_DF as Obs_to_DF \n",
    "import utils.get_VIIRS_SCA as get_VIIRS_SCA\n",
    "import utils.get_Precip as get_Precip\n",
    "import utils.get_Seasonality as get_Seasonality\n",
    "import utils.vegetation_processer as vegpro\n",
    "import utils.sturm_processer as stpro\n",
    "\n",
    "\n",
    "\n",
    "#make SWEMLv2.0 modeling domain for western USA\n",
    "# WY_list = ['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024'] #'2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024' - Trying the first bit to prove concept, then can expand\n",
    "WY_list = np.arange(2022,2024)\n",
    "output_res = 1000 #desired spatial resulution in meters (m)\n",
    "threshold = 10\n",
    "print(f\"The current session is using {WY_list} years, {output_res}m resolution, and {CPUS} CPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469889",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inputs for fetching ASO data for a region\n",
    "short_name = 'ASO_50M_SWE'\n",
    "directory = \"Raw_ASO_Data\"\n",
    "\n",
    "#Get ASO data, sometime sites will give error and break code, most times you can just rerun it using the data_processor sections below (e.g., comment out other parts\n",
    "for WY in WY_list:\n",
    "    #Convert ASO tifs to parquet\n",
    "    print(f\"Converting ASO images for WY: {WY}\")\n",
    "    folder_name = f\"{WY}/{directory}\"\n",
    "    data_processor = ASODataProcessing_v2() #note, 2019-5-1, 2019-06-11 seems to be bad, manually removed from SW region\n",
    "    data_processor.convert_tiff_to_parquet_multiprocess(folder_name, output_res, WY) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0597",
   "metadata": {},
   "source": [
    "## Get Snotel and CDEC in situ observations\n",
    "- Ideas - add nearest sites elevation, distance from cell, then can bypass sites with bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2a3c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only needed once. Other spatial resolutions can use the same data\n",
    "#Get in situ observations\n",
    "\n",
    "#make a list of dates to align with the ASO observations (they go as early as Jan-29 and as far out as the July-17)\n",
    "years = np.arange(2013,2025,1)#Needs to go 1yr out\n",
    "start_month_day = '10-01'\n",
    "end_month_day = '08-31'\n",
    "\n",
    "# observations \n",
    "get_InSitu_obs.Get_Monitoring_Data_Threaded_Updated(years, start_month_day, end_month_day, WY = True)\n",
    "\n",
    "#combine years\n",
    "get_InSitu_obs.combine_dfs(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329ea64",
   "metadata": {},
   "source": [
    "## Code for generating ML dataframe using nearest in situ monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde450",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GeoDF used to create a dataframe for ML model development. Its function is to connect in situ observations to gridded locations\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        #load snotel meta location data, use haversive function\n",
    "        GeoDF.fetch_snotel_sites_for_cellids(WY, output_res) # Using known up to date sites\n",
    "\n",
    "        # Get geophysical attributes for each site, need to see how to add output resolution\n",
    "        gdf = GeoDF.GeoSpatial(WY, output_res)\n",
    "\n",
    "        #use geodataframe with lat/long meta of all sites to determine slope, aspect, and elevation\n",
    "        metadf = GeoDF.extract_terrain_data_threaded(gdf, WY, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72349ac4",
   "metadata": {},
   "source": [
    "## Connect Snotel to each ASO obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a5df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rerun\n",
    "\n",
    "#Connect nearest snotel observations with ASO data, makes a parquet file for each date  -  test to see if this works - need to just load the SNOTEL file, not collect them as in the function\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        dates = []\n",
    "        manual = False\n",
    "        Obs_to_DF.Nearest_Snotel_2_obs_MultiProcess(WY, output_res, manual, dates) \n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7816b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Connect cell ids with ASO obs and snotel obs to geospatial features\n",
    "for WY in WY_list:\n",
    "    #path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        GeoDF.add_geospatial_threaded(WY, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facee8dc",
   "metadata": {},
   "source": [
    "## Get NASA VIIRS fraction snow covered area for each location \n",
    "\n",
    "* Make sure the code grabs all dates for each region, may have to run multiple times\n",
    "* run until \"No granules found for DATE, requesting data from NSIDC...\" no longer occurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317aadd-0020-403c-843b-c2543bdeae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_VIIRS_SCA.get_VIIRS_from_AWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fe64b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check to see if the VIIRS data is available locally, if not, get from CIROH AWS - I think all of this data is for the incorrect year...\n",
    "#get_VIIRS_SCA.get_VIIRS_from_AWS()\n",
    "\n",
    "#Connect VIIRS data to dataframes\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        get_VIIRS_SCA.augment_SCA_multiprocessing(WY, output_res, threshold)\n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96494a-ba0a-4376-9a98-2b9a632d4749",
   "metadata": {},
   "source": [
    "## Add seasonality metrics to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e511",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for WY in WY_list:\n",
    "    #process snotel sites to make \"snow hydrograph features\" to determine above/below average WY conditions\n",
    "    get_Seasonality.seasonal_snotel()\n",
    "\n",
    "\n",
    "    #get the Day of season metric for each dataframe\n",
    "    get_Seasonality.add_Seasonality(WY, output_res, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad085308",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use Sturm's snow classification as features within model framework\n",
    "\n",
    "Using the originally created env, it looks like the rasterio package does not contain the correct ECS driver. Trying to address this with conda install conda-forge::rasterio in my SWEML_310 env from the shell in CHPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4c09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#download sturm data\n",
    "stpro.get_Sturm_data()\n",
    "\n",
    "for WY in WY_list:\n",
    "    print(WY)\n",
    "    input_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    sturm_file = f\"{HOME}/data/SnowClassification/SnowClass_NA_300m_10.0arcsec_2021_v01.0.tif\" #https://nsidc.org/data/nsidc-0768/versions/1\n",
    "    output_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Sturm_Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    stpro.process_sturm_data_for_files(input_directory, sturm_file, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90eacf-432a-4d0b-a804-5cdfd62bfa0a",
   "metadata": {},
   "source": [
    "## Add vegetation data to the dataframe from the North American land Cover Management Systemoutput_path\n",
    "\n",
    "This script needs to be multiprocessed, too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bed310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get data\n",
    "url = \"http://www.cread_parquetiles/atlas_layers/1_terrestrial_ecosystems/1_01_0_land_cover_2020_30m/usa_land_cover_2020v2_30m_tif.zip\"\n",
    "output_path = f\"{HOME}/data/LandCover/\"\n",
    "file = \"usa_land_cover_2020v2_30m_tif.zip\" \n",
    "vegpro.get_data(url, output_path, file)\n",
    "#unzip the file is not already done\n",
    "#vegpro.unzip_LC_data(output_path, file)\n",
    "#output = 1000 \n",
    "\n",
    "for WY in WY_list:\n",
    "    print(WY)\n",
    "    input_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Sturm_Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    vegetation_file = f\"{HOME}/data/LandCover/usa_land_cover_2020v2_30m_tif/USA_NALCMS_landcover_2020v2_30m/data/USA_NALCMS_landcover_2020v2_30m.tif\"\n",
    "    output_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    vegpro.process_vegetation_data_for_files(input_directory, vegetation_file, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf01e14-94ac-4c2e-a143-dc9215dda1c3",
   "metadata": {},
   "source": [
    "## Get Daymet Precipitation for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee65242-bebf-4080-915e-75952f8275b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WY_list = [2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d07e61-a124-415c-b2d6-7063c9afbe16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'Daymet'\n",
    "\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        # get_Precip.get_daymet_precip(WY, output_res, years)\n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}, {path}\")\n",
    "\n",
    "    #Connect precipitation to processed DFs\n",
    "    get_Precip.Make_Precip_DF(WY, output_res, threshold, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79ca66-c06f-415f-a3a8-fb2f882962b2",
   "metadata": {},
   "source": [
    "## Get NLDAS Precipitation for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a59828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "note*, if using python > 3.9, you will likely need to change the ee package to from io import StringIO\n",
    "sometimes there will be an ASO file that is inproperly named in the ASO yr folder, it typically also makes a ,ipynb checkpoint that crash the code\n",
    "'''\n",
    "#gets precipitation for each location, accumulates it through the water year\n",
    "#This step could be made much more efficient by collecting all of the tiles in one step, then multiprocessing later\n",
    "\n",
    "#set start/end date for a water year\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "dataset = 'NLDAS'\n",
    "\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        # get_Precip.get_precip_threaded(WY, output_res, years)\n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}, {path}\")\n",
    "\n",
    "    #Connect precipitation to processed DFs\n",
    "    get_Precip.Make_Precip_DF(WY, output_res, threshold, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e51b09",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "* Explore why errors in precip sites above\n",
    "* add in situ obs - seasonality based on the historical neareste x monitoring stations - like a historical average to-date swe value unit hydrograph based on the day of year? This will include a historical time of year of normal swe value and a swe value of year compared to normal\n",
    "* albedo metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "region = 'Southwest'\n",
    "output_res = '300'\n",
    "\n",
    "dfpath = f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution\"\n",
    "\n",
    "SWmeta = pd.read_parquet(f\"{dfpath}/{region}_metadata.parquet\")\n",
    "\n",
    "import UpdateDataFrame\n",
    "\n",
    "#need to update the topographic features for every dataframe\n",
    "output_res = '300'\n",
    "training_cats = ['Obsdf']\n",
    "fSCA = '' #'20_fSCA_Thresh'\n",
    "\n",
    "\n",
    "for training_cat in training_cats:\n",
    "    print(training_cat)\n",
    "\n",
    "    for region in region_list:\n",
    "        print(region)\n",
    "        dfpath = f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution\"\n",
    "        #file to be used to updated training DF\n",
    "        updatefile = pd.read_parquet(f\"{dfpath}/{region}_metadata.parquet\")\n",
    "\n",
    "\n",
    "        #Update Dataframe\n",
    "        UpdateDataFrame.updateTrainingDF(region, output_res, training_cat, fSCA, updatefile)\n",
    "\n",
    "trainfile = pd.read_parquet(f\"{dfpath}/{training_cat}/{fSCA}/Sturm_Season_Precip_VIIRS_GeoObsDF_20150406.parquet\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def SpatialAnalysis(EvalDF):\n",
    "    #Convert to a geopandas DF\n",
    "    Pred_Geo = gpd.GeoDataFrame(EvalDF, geometry = gpd.points_from_xy(EvalDF.cen_lon, EvalDF.cen_lat))\n",
    "\n",
    "    Pred_Geo.plot(column='Elevation_m',\n",
    "                  legend=False,\n",
    "                )\n",
    "    \n",
    "SpatialAnalysis(trainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f70f64-8097-451d-ac30-e511a895e585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_310",
   "language": "python",
   "name": "sweml_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
