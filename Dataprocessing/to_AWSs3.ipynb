{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push/Pull to AWS\n",
    "* Training DF files\n",
    "* then push other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/uufs/chpc.utah.edu/common/home/civil-group1/Johnson/SWEMLv2.0/Dataprocessing/AWSaccessKeys.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# HOME = os.chdir('../..')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# HOME = os.getcwd()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m KEYPATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAWSaccessKeys.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m ACCESS \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mHOME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mKEYPATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#start session\u001b[39;00m\n\u001b[1;32m     23\u001b[0m SESSION \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mSession(\n\u001b[1;32m     24\u001b[0m     aws_access_key_id\u001b[38;5;241m=\u001b[39mACCESS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccess key ID\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     25\u001b[0m     aws_secret_access_key\u001b[38;5;241m=\u001b[39mACCESS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSecret access key\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/SWEML_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/SWEML_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/SWEML_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/SWEML_310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/SWEML_310/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/uufs/chpc.utah.edu/common/home/civil-group1/Johnson/SWEMLv2.0/Dataprocessing/AWSaccessKeys.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#set path directory\n",
    "\n",
    "#load access key\n",
    "#HOME = os.path.expanduser('~')\n",
    "HOME = os.getcwd()\n",
    "# HOME = os.chdir('../..')\n",
    "# HOME = os.getcwd()\n",
    "KEYPATH = \"AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'national-snow-model'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/uufs/chpc.utah.edu/common/home/civil-group1/Johnson'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DF regional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload nearest-snotel dictionaries\n",
    "regionlist = ['SouthernRockies', 'Southwest', 'Northwest']\n",
    "output_res = 300\n",
    "\n",
    "for region in regionlist:\n",
    "    print(region)\n",
    "    head_folder = f\"SWEMLv2.0/data/TrainingDFs/{region}/300M_Resolution\"\n",
    "    head_folder_dir = f\"{HOME}/{head_folder}\"\n",
    "\n",
    "    #Get files only\n",
    "    files = [f for f in listdir(head_folder_dir) if isfile(join(head_folder_dir, f))]\n",
    "    # for file in tqdm_notebook(files):\n",
    "    #     S3.meta.client.upload_file(Filename= f\"{head_folder_dir}{file}\", Bucket=BUCKET_NAME, Key=f\"{head_folder}{file}\")\n",
    "\n",
    "    #get list of directories to download\n",
    "    dirs = [ f.path for f in os.scandir(head_folder_dir) if f.is_dir() ]\n",
    "    for dir in dirs:\n",
    "        print(f\"Folder name: {dir.split('300M_Resolution/')[-1]}\")\n",
    "        awsfolderpath = f\"SWEMLv2.0{dir.split('SWEMLv2.0')[-1]}\"\n",
    "        headfolderpath = f\"{HOME}/{awsfolderpath}\"\n",
    "        files = [f for f in listdir(headfolderpath) if isfile(join(headfolderpath, f))]\n",
    "        # for file in tqdm_notebook(files):\n",
    "        #      S3.meta.client.upload_file(Filename= f\"{headfolderpath}/{file}\", Bucket=BUCKET_NAME, Key=f\"{awsfolderpath}/{file}\")\n",
    "        #     # print(f\"{headfolderpath}/{file}\")\n",
    "\n",
    "        # # #Get subfolders\n",
    "        subdirs = [ f.path for f in os.scandir(headfolderpath) if f.is_dir() ]\n",
    "        for dir in subdirs:\n",
    "            print(f\"Folder name: {dir.split('300M_Resolution/')[-1]}\")\n",
    "            awsfolderpath2 = f\"SWEMLv2.0{dir.split('SWEMLv2.0')[-1]}\"\n",
    "            headfolderpath2 = f\"{HOME}/{awsfolderpath2}\"\n",
    "            files = [f for f in listdir(headfolderpath2) if isfile(join(headfolderpath2, f))]\n",
    "            for file in tqdm_notebook(files):\n",
    "                S3.meta.client.upload_file(Filename= f\"{headfolderpath2}/{file}\", Bucket=BUCKET_NAME, Key=f\"{awsfolderpath2}/{file}\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Northwest\n",
      "['SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/ASO_meta.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/GeoObsDFs/GeoObsdfs_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/GeoObsDFs/GeoObsdfs_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Northwest_metadata.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Obsdf/20160208_ObsDF.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Obsdf/20160329_ObsDF.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Season_Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Season_Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Sturm_Season_Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Sturm_Season_Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/VIIRSGeoObsDFs/20_fSCA_Thresh/VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/VIIRSGeoObsDFs/20_fSCA_Thresh/VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Vegetation_Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Vegetation_Sturm_Season_Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Vegetation_Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Vegetation_Sturm_Season_Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/nearest_SNOTEL.pkl']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392d736a384f446b98945eb8b76e8aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "Northwest\n",
      "['SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/ASO_meta.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/GeoObsDFs/GeoObsdfs_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/GeoObsDFs/GeoObsdfs_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Northwest_metadata.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Obsdf/20160208_ObsDF.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Obsdf/20160329_ObsDF.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Season_Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Season_Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Sturm_Season_Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Sturm_Season_Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/VIIRSGeoObsDFs/20_fSCA_Thresh/VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/VIIRSGeoObsDFs/20_fSCA_Thresh/VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Vegetation_Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Vegetation_Sturm_Season_Precip_VIIRS_GeoObsDF_20160208.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/Vegetation_Sturm_Seasonality_PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Vegetation_Sturm_Season_Precip_VIIRS_GeoObsDF_20160329.parquet', 'SWEMLv2.0/data/TrainingDFs/Northwest/500M_Resolution/nearest_SNOTEL.pkl']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255d962aa6ee45bba828d414e859d3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n",
      "File already present...\n"
     ]
    }
   ],
   "source": [
    "#Upload nearest-snotel dictionaries\n",
    "regionlist = ['Northwest','SouthernRockies', 'Southwest' ]\n",
    "output_res = 500\n",
    "\n",
    "#Get data\n",
    "for region in regionlist:\n",
    "    print(region)\n",
    "    path = f\"SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution\"\n",
    "\n",
    "    #Make directory if it does not exist\n",
    "    if not os.path.exists(f\"{HOME}/{path}\"):\n",
    "        print(\"Path not present, making\")\n",
    "        os.makedirs(f\"{HOME}/{path}\", exist_ok=True)\n",
    "    \n",
    "    #Identify the potential files to download\n",
    "    files = [objects.key for objects in BUCKET.objects.filter(Prefix=path)]\n",
    "    #print(files)\n",
    "    \n",
    "    for file in tqdm_notebook(files):\n",
    "        #Make directory if it does not exist\n",
    "        if not os.path.exists(f\"{HOME}/{path}\"):\n",
    "            print(\"Path not present, making\")\n",
    "        os.makedirs( os.path.dirname(f\"{HOME}/{file}\"), exist_ok=True)\n",
    "       \n",
    "        #check to see if the file is there\n",
    "        if os.path.exists(f\"{HOME}/{file}\") == False:\n",
    "            print('Downloading file')\n",
    "            S3.Bucket(BUCKET_NAME).download_file(file, f\"{HOME}/{file}\")\n",
    "\n",
    "        else:\n",
    "            print('File already present...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.dirname(f\"{HOME}/Test/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/uufs/chpc.utah.edu/common/home/civil-group1/Johnson'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Northwest\n",
      "Northwest\n"
     ]
    }
   ],
   "source": [
    "#Upload nearest-snotel dictionaries\n",
    "regionlist = ['Northwest', 'Northwest'] #'SouthernRockies', 'Southwest' ]\n",
    "output_res = 500\n",
    "\n",
    "#Get data\n",
    "for region in regionlist:\n",
    "    print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_310",
   "language": "python",
   "name": "sweml_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
